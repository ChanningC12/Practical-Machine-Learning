Practical Machine Learning - Week 1
============================================================
##### Identify patients who will be admitted to a hospital within the next year, using historical claims data [link](http://www.heritagehealthprize.com/hhp)  
[Kaggle](http://www.kaggle.com)

##### components of a predictor  
- question -> input data -> features -> algorithm -> parameters -> evaluation  

##### relative order of importance  
- question > data > feature > algorithm
- Often more data > better models

##### Properties of good features
- Lead to data compression
- Retain relevant information
- Are created based on expert application knowledge

##### Common mistakes
- Trying to automate feature selection
- Not paying attention to data-specific quirks
- Throwing away information unnecessarily

##### The "Best" Machine Learning Method
- Interpretable
- Simple
- Accurate
- Fast (to train and test)
- Scalable

##### Prediction is about accuracy tradeoffs
- Interpretability versus accuracy
- Speed versus accuracy
- Simplicity versus accuracy
- Scalability versus accuracy

##### SPAM Example
```{r}
library(kernlab)
data(spam)
head(spam)
```

##### Plot the density of "your"
```{r}
plot(density(spam$your[spam$type=="nonspam"]),
     col="blue",main="",xlab="Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]),col="red")
```

##### Find a value C, if frequency of 'yours'>C, then predict "spam"
```{r}
plot(density(spam$your[spam$type=="nonspam"]),
     col="blue",main="",xlab="Frequency of 'your'")
lines(density(spam$your[spam$type=="spam"]),col="red")
abline(v=0.5,col="black")
```
  
##### prediction based on 0.5 threshold
```{r}
prediction<-ifelse(spam$your>0.5,"spam","nonspam")
table(prediction,spam$type)/length(spam$type)
```

  
##### In sample and out of sample error
- In sample error: the error rate you get on the same data set you used to build your predictor. Resubstitution error
- Out of sample error: The eror rate you get on a new data set. Generalization error

```{r}
set.seed(333)
smallSpam = spam[sample(dim(spam)[1],size=10),]
spamLabel = (smallSpam$type=="spam")*1+1
plot(smallSpam$capitalAve,col=spamLabel)
```

##### Split data into
- Training: build the model
- Testing: validate the model (can not use multiple model on test set, that's training the model)
- Validation: Also use to validate the model (refine model on test set and validate on validation set)

##### Common error measures
1. Mean squared error (or root mean squared error), continuous data, sensitive to outliers
2. Median absolute deviation, continuous data, often more robust. Median of the distance between observed value and predicted value, absolute value
3. Sensitivity (recall), if you want few missed positives, fewer FN
4. Specificity, if you want few negatives called positives, FP
5. Accuracy, weights false positives / negatives equally
6. Concordance, kappa

##### Cross-validation Approach
1. Use the training set
2. Split it into trainin/test sets
3. Build a model on the training set
4. Evaluate on the test set
5. Repeat and average the estimated errors

##### Considerations
1. For time series data, data must be used in chunks
2. For k-fold cross validation
- Large k, less bias (predicted to truth), more variance
- Smaller k, more bias, less variance
3. Random sampling must be done without replacement
4. Random sampling with replacement is the bootstrap
5. If you cross-validate to pick predictors estimate you must estimate errors on independent data









